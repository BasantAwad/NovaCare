NovaBot: An AI-Powered Assistant Rover for the
Disabled
Graduation Project Proposal
Ramez Asaad [Your Teammates’ Names]
Alamein International University - Faculty of Computer and AI Sciences
November 27, 2025
Contents
1 Project Information 4
1.1 Project Title . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Project Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Project Team . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4 Supervisors / Advisors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.5 Project Category . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.6 Keywords . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.7 Deliverables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.8 Project Duration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2 Introduction 6
2.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.4 Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.5 Significance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Literature Review 8
3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.2 Assistive Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.3 AI in Human-Robot Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.4 Multimodal Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.5 Context Awareness and Emotion Recognition . . . . . . . . . . . . . . . . . . . . . . 9
3.6 Gaps in Existing Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4 Project Description 10
4.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.2 System Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3 System Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3.1 Hardware Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3.2 Software Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.3.3 Cloud Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.4 System Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.5 System Architecture Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
4.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5 Technologies and Tools 13
5.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.2 Hardware Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.2.1 Core Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.2.2 Sensors and Peripherals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.3 Software Technologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1
5.3.1 Artificial Intelligence and Machine Learning . . . . . . . . . . . . . . . . . . . 14
5.3.2 Robotics and Navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5.3.3 Cloud and Backend Technologies . . . . . . . . . . . . . . . . . . . . . . . . . 14
5.4 Development and Testing Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
5.5 Justification of Technology Choices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
5.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
6 Requirements Analysis 16
6.1 Functional Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6.1.1 Safety & Emergency Response . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6.1.2 Health & Wellness Management . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6.1.3 Multimodal Interaction & Companionship . . . . . . . . . . . . . . . . . . . . 17
6.1.4 Autonomous Navigation & Visual Assistance . . . . . . . . . . . . . . . . . . . 17
6.1.5 User & System Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6.2 Non-Functional Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
6.2.1 Performance Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
6.2.2 Reliability and Availability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
6.2.3 Security Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
6.2.4 Usability and Accessibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.2.5 Scalability and Maintainability . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.2.6 Safety and Compliance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.2.7 Portability and Compatibility . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
6.2.8 Data Quality and Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
6.2.9 Ethical and Privacy Requirements . . . . . . . . . . . . . . . . . . . . . . . . . 20
6.3 Business Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
6.3.1 Market & Social Impact Requirements . . . . . . . . . . . . . . . . . . . . . . 20
6.3.2 Stakeholder Value Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6.3.3 Technical Excellence & Innovation Requirements . . . . . . . . . . . . . . . . . 21
6.3.4 Scalability & Sustainability Requirements . . . . . . . . . . . . . . . . . . . . 22
6.3.5 Educational & Research Contribution Requirements . . . . . . . . . . . . . . . 22
6.3.6 Regulatory, Safety & Compliance Requirements . . . . . . . . . . . . . . . . . 23
6.3.7 Strategic Positioning & Long-Term Vision Requirements . . . . . . . . . . . . 23
6.4 Business Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
6.4.1 Authentication & Access Control . . . . . . . . . . . . . . . . . . . . . . . . . 24
6.4.2 Communication & Accessibility . . . . . . . . . . . . . . . . . . . . . . . . . . 24
6.4.3 Safety & Emergency Response . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
6.4.4 Data Privacy & Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
6.4.5 AI Ethics & Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
6.4.6 System Maintenance & Operations . . . . . . . . . . . . . . . . . . . . . . . . 25
7 System Design 26
7.1 Use Case Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
7.2 Architecture Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
7.2.1 High-Level Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
7.2.2 Composite Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
7.3 Data Flow Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
7.3.1 Level 0 DFD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
7.3.2 Level 1 DFD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
7.4 Sequence Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
7.5 Class Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
7.6 Entity Relationship Diagram (ERD) . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
7.7 User Stories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2
7.8 Traceability Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
8 Testing Methodologies 33
8.1 Testing Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
8.2 Test Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
8.3 Unit Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
8.4 Integration Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
8.5 System Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
8.6 Acceptance Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
8.7 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
8.8 Testing Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
9 Project Management 34
9.1 Timeline / Gantt Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
9.2 Team Roles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
9.3 Risk Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
10 Ethical and Legal Considerations 35
10.1 Timeline / Gantt Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
10.2 Team Roles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
10.3 Risk Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
11 Conclusion 36
12 References 37
3
Chapter 1
Project Information
1.1 Project Title
NovaBot: An AI-Powered Assistant Rover for the Disabled
1.2 Project Summary
NovaBot is an AI-powered assistant rover designed to aid individuals with physical or sensory dis-
abilities through intelligent, multimodal interaction. It integrates advanced AI models, including a
Large Language Model for conversation, speech and sign language recognition, and computer vision
for environmental awareness. NovaBot can autonomously navigate, follow the user, detect obstacles,
and adapt to the user’s preferred mode of communication (voice, ASL, or touch). With features
such as emotion and fall detection, traffic rule awareness, and live guardian monitoring, NovaBot
enhances independence, safety, and companionship for users in their daily lives.
1.3 Project Team
Name Student ID Program
Basant Awad 22101405 CS
Nadira El-Sirafy 22101377 CS
Nourine Yasser 22101109 AIS
Mohamed Mustafa 22101336 AIS
Ramez Asaad 22100506 AIS
1.4 Supervisors / Advisors
Name Title / Position Department
Dr. Ahmed Shalaby Project Supervisor Computer Science Depart-
ment
1.5 Project Category
Category:AI and Robotics
Type:Research and Development (R&D)
Field:Artificial Intelligence, Human-Robot Interaction, Assistive Technology
4
1.6 Keywords
Artificial Intelligence, Robotics, Assistive Technology, Computer Vision, LLM, Accessibility, Human-
Robot Interaction
1.7 Deliverables
•NovaBot physical prototype (rover)
•NovaBot software system (AI modules + control interface)
•Guardian monitoring web/mobile dashboard
•Final report and testing documentation
•Presentation and video demonstration
1.8 Project Duration
Phase Duration Description
Phase 1 – Research &
Design
2 weeks Define problem, scope, and architecture
Phase 2 – System De-
sign
2 weeks Prepare all UML and architecture diagrams
Phase 3 – Develop-
ment
6 weeks Implement hardware, AI models, and soft-
ware
Phase 4 – Testing 3 weeks Conduct integration and system testing
Phase 5 – Documenta-
tion & Presentation
2 weeks Finalize report, prepare for defense
5
Chapter 2
Introduction
2.1 Background
Advances in artificial intelligence, robotics, and computer vision have made it possible to develop
intelligent systems that can interact naturally with humans. Among the most impactful applica-
tions of these technologies are assistive robots: machines designed to enhance the independence and
quality of life of individuals with disabilities. Many people with physical, sensory, or communication
impairments face daily challenges in mobility, social interaction, and access to information. While
smart devices and voice assistants have improved accessibility, they remain limited in physical inter-
action and contextual understanding.
NovaBot aims to bridge this gap by combining artificial intelligence, speech and sign recognition,
computer vision, and autonomous navigation into one assistive rover. By integrating these technolo-
gies, NovaBot serves as both a functional assistant and a companion, capable of understanding the
user’s environment and communicating through multiple modalities.
2.2 Problem Statement
Individuals with disabilities often rely on fragmented solutions to perform daily tasks, voice assistants
for reminders, wheelchairs for mobility, and caregivers for communication or safety. These systems
do not communicate or adapt to the user’s context, leading to dependency, reduced independence,
and limited autonomy.
There is a clear need for an intelligent, unified system that can assist users in real-world environments,
one that can perceive, communicate, and act with awareness of human needs and surroundings.
NovaBot addresses this challenge by providing a multi-sensory, adaptive robotic assistant capable of
understanding natural communication methods and responding in real time.
2.3 Objectives
The main objective of NovaBot is to design and implement an AI-powered assistant rover that
supports individuals with disabilities through intelligent interaction and assistance.
Specific objectives:
•Develop a multimodal communication system integrating voice, text, and sign language recog-
nition.
•Implement a vision-based navigation system for autonomous movement and obstacle avoidance.
•Integrate emotion and fall detection to enhance safety and empathetic response.
6
•Develop a guardian monitoring interface for live tracking and alerts.
•Ensure the system operates efficiently on embedded hardware suitable for real-time perfor-
mance.
2.4 Scope
The project focuses on the design and development of an assistive robotic rover prototype that can
operate in indoor environments such as homes, rehabilitation centers, and campuses. The system
will include:
•A physical rover equipped with sensors and cameras for environmental awareness.
•An AI-powered software system capable of perception, communication, and decision-making.
•A user interface for configuration and guardian monitoring.
NovaBot’s initial prototype will emphasize safety, communication, and assistance rather than heavy-
load carrying or advanced manipulation tasks. Future iterations may extend to outdoor navigation
and integration with IoT environments.
2.5 Significance
NovaBot represents a step toward inclusive technology, designed to empower, not replace, human
capability. By merging robotics and artificial intelligence for accessibility, it contributes to social
inclusion, independent living, and emotional well-being.
The project also serves as a practical demonstration of applying AI models, embedded systems, and
computer vision in a real-world context, aligning with the global push for assistive innovation and
human-centered design. It has the potential to inspire similar solutions in healthcare, rehabilitation,
and education, especially within developing regions where accessibility tools remain limited.
7
Chapter 3
Literature Review
3.1 Overview
Assistive robotics has become one of the most promising fields in the intersection of artificial intel-
ligence, computer vision, and human-computer interaction. Over the past decade, various research
projects and commercial prototypes have aimed to improve the quality of life for individuals with
disabilities through autonomous systems. This chapter provides an overview of existing technologies,
methodologies, and research efforts that inspired and informed the design of NovaBot.
3.2 Assistive Robotics
Assistive robots are designed to support individuals in daily activities, promote independence, and
enhance accessibility. These robots can be categorized into three main groups:
•Mobility Assistants:Robots that help with navigation or movement, such as smart wheelchairs
and robotic guides for the visually impaired.
•Social Assistants:Robots that provide companionship and communication support, such as
humanoid assistants and therapy robots.
•Service Robots:Robots that perform specific physical tasks, such as carrying objects or
retrieving items.
Examples includePepperby SoftBank Robotics, designed for social interaction, andGuideCane,
which assists visually impaired users with navigation. However, many of these solutions focus on
either mobility or communication, not both, and lack contextual awareness or emotional adaptability.
3.3 AI in Human-Robot Interaction
Recent advances in AI have enabled robots to better understand and interact with humans. Large
Language Models (LLMs) such as OpenAI’s GPT and Google’s Gemini have shown remarkable
capabilities in natural conversation and context understanding. When integrated into robotics,
these models allow for dynamic and context-aware communication.
Computer vision plays a complementary role, enabling robots to interpret their surroundings and
respond intelligently. State-of-the-art object detection models like YOLOv8, SSD, and EfficientDet
can recognize and track multiple objects in real time, forming the foundation for navigation, obstacle
avoidance, and gesture recognition.
8
3.4 Multimodal Communication
One of the core challenges in assistive technology is creating systems that accommodate diverse user
needs. People with speech or hearing impairments may rely on sign language, text, or visual cues to
communicate. Research on multimodal systems — combining speech, vision, and text — shows that
integrating multiple input and output channels greatly enhances accessibility and user experience.
Projects such asDeepASLandSignAllhave demonstrated the feasibility of automatic sign language
recognition using deep learning models trained on camera feeds. Similarly, advancements in Text-to-
Speech (TTS) and Speech-to-Text (STT) systems, such as Whisper and Tacotron, have made voice
interaction more natural and responsive.
3.5 Context Awareness and Emotion Recognition
Contextual understanding is essential for building empathetic and safe assistive systems. Emotion
recognition systems leverage computer vision and affective computing techniques to analyze facial
expressions, tone of voice, or behavioral cues. Studies show that emotionally responsive robots can
improve user trust, engagement, and comfort, especially among individuals with disabilities or the
elderly.
Furthermore, fall detection systems, often built using accelerometers, cameras, or pose estimation
models, play a crucial role in ensuring user safety. When integrated into a mobile platform, they can
trigger alerts to caregivers or guardians in real time.
3.6 Gaps in Existing Solutions
Although existing assistive robots and AI systems have achieved significant progress, several gaps
remain:
•Most assistive robots focus on a single mode of interaction, limiting accessibility for users with
multiple disabilities.
•Few systems integrate both conversational AI and computer vision for real-time environmental
understanding.
•Existing commercial robots often lack affordable hardware designs suitable for real-world use
in developing regions.
•Emotion, fall detection, and safety systems are typically treated as separate modules rather
than integrated into a unified framework.
These limitations highlight the need for a holistic, multimodal, and context-aware robotic assis-
tant; one that merges perception, communication, and mobility to offer meaningful, human-centered
support.
3.7 Summary
This literature review establishes that while assistive robotics and AI have made significant strides,
there remains an unmet need for an integrated, accessible, and adaptive system for people with
disabilities. NovaBot aims to fill this gap by unifying multimodal communication, autonomous
mobility, emotion detection, and safety monitoring into one intelligent rover. By combining these
domains, NovaBot represents a step toward a new generation of assistive technology that prioritizes
empathy, inclusivity, and independence.
9
Chapter 4
Project Description
4.1 Overview
NovaBot is an AI-powered assistant rover designed to support individuals with disabilities through
intelligent communication, perception, and autonomous mobility. It combines artificial intelligence,
robotics, and computer vision to provide a holistic assistive experience.
Unlike conventional assistive devices that rely on single modes of input such as voice or text, NovaBot
offers a multimodal interaction system, enabling users to communicate via speech, text, or sign
language. The robot adapts its communication and assistance style based on the user’s disability,
ensuring accessibility and inclusivity.
4.2 System Concept
NovaBot acts as a personal companion and assistant, capable of perceiving its surroundings, un-
derstanding user intent, and responding accordingly. The rover integrates multiple AI models and
hardware subsystems that allow it to:
•Communicate naturally using speech and text.
•Recognize sign language and gestures from camera input.
•Navigate autonomously while avoiding obstacles and following its user.
•Detect and interpret human emotions for empathetic interaction.
•Identify falls or emergencies and alert the guardian system in real time.
•Provide entertainment and companionship through multimedia interaction.
4.3 System Components
The NovaBot system consists of three main layers: hardware, software, and cloud integration.
4.3.1 Hardware Layer
The hardware layer contains all physical and sensory components of the rover:
•Chassis and Motors:Supports omnidirectional movement for smooth indoor navigation.
•Cameras and Sensors:Used for object detection, human tracking, obstacle avoidance, and
environmental perception.
10
•Microphone Array:Captures voice commands for speech recognition.
•Touchscreen Display:Provides visual interface and text-based interaction.
•Speakers:Output system responses, alerts, and entertainment audio.
•Edge Computing Unit:(e.g., NVIDIA Jetson or Raspberry Pi) processes AI inference and
navigation tasks in real time.
4.3.2 Software Layer
This layer integrates the AI models, perception modules, and control algorithms:
•Large Language Model (LLM):Handles dialogue, context understanding, and reasoning.
•Speech-to-Text (STT) Model:Converts user speech into text commands.
•Text-to-Speech (TTS) Model:Produces natural voice output.
•ASL Recognition Model:Interprets sign language from camera feed using computer vision.
•Computer Vision Model:Performs object detection, emotion recognition, and environmen-
tal analysis.
•Navigation System:Controls autonomous movement, user following, and obstacle avoidance.
•Safety and Alert System:Includes fall detection and real-time guardian notifications.
4.3.3 Cloud Layer
The cloud layer supports monitoring, storage, and communication:
•Cloud Database:Stores user profiles, configurations, logs, and AI-generated context data.
•Guardian Dashboard:Allows remote live camera access and monitoring.
•Data Analytics:Tracks usage patterns and robot performance for continuous improvement.
4.4 System Features
NovaBot integrates a wide range of features designed to address the needs of individuals with different
disabilities:
•Multimodal Interaction:Communication through speech, text, or ASL depending on the
user’s needs.
•Adaptive Input and Output:The robot automatically selects the most suitable input/output
mode (voice, text, or visual display).
•Autonomous Mobility:Ability to navigate, follow the user, and avoid obstacles in real time.
•Emotion and Mood Detection:Enhances interaction by recognizing facial expressions and
adapting responses.
•Fall Detection:Monitors and detects sudden falls, triggering guardian alerts immediately.
•Guardian Monitoring System:Live camera streaming and safety notifications accessible
via web or mobile app.
11
•Entertainment and Assistance:Provides media playback, reminders, and companionship
to reduce isolation.
•Traffic Compliance:Follows basic indoor and outdoor navigation rules for safe movement.
4.5 System Architecture Overview
NovaBot’s architecture follows a modular design that enables distributed processing between the
rover and the cloud. The architecture can be summarized as:
•Input Layer:Captures multimodal inputs (speech, camera feed, touchscreen).
•Processing Layer:Performs AI inference (LLM reasoning, object detection, emotion analy-
sis).
•Control Layer:Generates movement commands and system responses based on AI output.
•Output Layer:Delivers responses through voice, text, or visual feedback.
•Cloud Layer:Handles monitoring, data storage, and remote control.
This modular approach ensures scalability, allowing the integration of new AI models and hardware
upgrades without reconfiguring the entire system.
4.6 Summary
NovaBot integrates multiple disciplines: artificial intelligence, robotics, human-computer interaction,
and embedded systems into a unified solution for assistive technology. Its core innovation lies in its
ability to perceive, understand, and respond to users naturally and empathetically, regardless of
their disability. The project not only advances technical learning but also contributes meaningfully
to accessibility, inclusion, and human-centered design.
12
Chapter 5
Technologies and Tools
5.1 Overview
NovaBot integrates a wide range of technologies spanning artificial intelligence, embedded systems,
and cloud computing. The selection of each technology was based on performance, compatibility,
scalability, and ease of integration within a real-time assistive robotics environment.
This chapter outlines the hardware and software technologies that will be utilized throughout the
design, implementation, and testing of the NovaBot system.
5.2 Hardware Components
5.2.1 Core Hardware
The hardware components are anchored by the Hiwonder JetAuto robot kit, which forms the mobile
platform for NovaBot. Key features of the kit include:
•Omnidirectional movement chassis with mecanum-wheels, enabling smooth lateral movement
and agile navigation in tight indoor spaces.
•A programmable edge computing unit (e.g., NVIDIA Jetson Nano or Raspberry Pi 5) pre-
integrated into major JetAuto versions for ROS1/ROS2 compatibility.
•A 3D depth camera and LiDAR (e.g., SLAMTEC A1) for simultaneous localization and map-
ping (SLAM), obstacle detection and navigation planning.
•7-inch touchscreen display, built-in microphone array, speaker system, and expansion board for
peripherals and sensors.
Since NovaBot’s assistive functionality requires reliable mobility, perception and adaptability, the
JetAuto kit provides a strong foundation; reducing build time and offering a flexible platform that
supports our AI, vision and interaction modules.
5.2.2 Sensors and Peripherals
In addition to the standard kit components, the following sensors and peripherals will be integrated
for NovaBot:
•HD camera module (RGB + depth) for computer vision, ASL recognition, and environment
monitoring.
•Ultrasonic sensors and/or IR sensors for close-range obstacle detection and fall detection sup-
port.
13
•Microphone array (six-microphone ring) for far-field voice recognition and audio localization
(supported by some JetAuto Pro versions).
•Touchscreen display and speaker output (leveraging the JetAuto kit’s optional screen) for user
interface, voice output and accessibility.
5.3 Software Technologies
The software stack powering NovaBot includes multiple layers: AI models, computer vision libraries,
control algorithms, and cloud services.
5.3.1 Artificial Intelligence and Machine Learning
•Large Language Model (LLM):Used for conversation, reasoning, and task understanding.
Models such as GPT-based APIs or open-source alternatives (e.g., Llama 3) will be explored.
•Speech-to-Text (STT):Implemented using Whisper or Google Speech API for accurate
voice transcription.
•Text-to-Speech (TTS):Powered by models like Tacotron or Amazon Polly for natural speech
output.
•ASL Recognition:Built using a CNN or MediaPipe-based model trained on sign language
datasets.
•Object Detection:Implemented using YOLOv8 for real-time frame analysis and obstacle
identification.
•Emotion Detection:Based on facial expression analysis using OpenCV and deep learning.
•Fall Detection:Using pose estimation models (e.g., BlazePose) and motion analysis from
camera feed.
5.3.2 Robotics and Navigation
•ROS (Robot Operating System):Provides communication between hardware nodes, sen-
sor fusion, and navigation control.
•OpenCV:Used for image and video frame processing.
•TensorFlow / PyTorch:Frameworks for training and running deep learning models on the
Jetson board.
•Arduino / Microcontroller Code:Controls low-level sensors, motors, and feedback sys-
tems.
5.3.3 Cloud and Backend Technologies
•Database:Hybrid architecture: PostgreSQL (SQL) for structured data and MongoDB (NoSQL)
for AI context and logs.
•Backend Framework:Node.js or Flask API to manage data exchange between robot and
cloud services.
•Cloud Platform:AWS, Google Cloud, or Render for deployment, storage, and guardian
dashboard hosting.
14
•WebSocket / MQTT:Real-time communication between robot, guardian interface, and
monitoring server.
5.4 Development and Testing Tools
•Development Environment:VS Code, Jupyter Notebook, and Arduino IDE.
•Version Control:Git and GitHub for collaborative development and source code manage-
ment.
•Simulation and Prototyping:Gazebo or Webots for testing navigation algorithms before
hardware integration.
•Design Tools:Figma for interface design and Draw.io for system and architecture diagrams.
•Testing Frameworks:PyTest for software modules, ROS testing utilities for motion control,
and manual evaluation for real-world performance.
5.5 Justification of Technology Choices
Each chosen technology was selected based on performance, compatibility, and accessibility:
•Jetson / Raspberry Pi:Offers the balance between processing power and affordability
required for edge AI computation.
•YOLOv8 and MediaPipe:Provide lightweight yet high-accuracy real-time detection and
recognition capabilities.
•ROS Integration:Enables modular design and communication between perception, planning,
and control nodes.
•Hybrid Database:Combines relational reliability with flexible storage for AI-driven and
unstructured data.
•Cloud Monitoring:Ensures the guardian can remotely monitor NovaBot safely and in real
time.
5.6 Summary
The technologies and tools selected for NovaBot were chosen to achieve real-time performance,
scalability, and user accessibility. The combination of embedded AI, robust robotics frameworks, and
cloud integration allows NovaBot to operate intelligently and autonomously, providing dependable
assistance tailored to the needs of each user.
15
Chapter 6
Requirements Analysis
6.1 Functional Requirements
NovaBot’s functional requirements define the core capabilities and functions that the system must
provide to meet the needs of users with disabilities. These requirements are organized into five
primary groups that address safety, health, interaction, navigation, and system management.
6.1.1 Safety & Emergency Response
This group of requirements governs the system’s primary safety-critical functions.
•FR-1.1:The system shall continuously monitor the Primary User’s state to detect a fall event.
•FR-1.2:Upon detection of a fall (per FR-1.1), the system shall automatically initiate an
emergency alert sequence.
•FR-1.3:The system shall continuously monitor the environment for hazards, specifically
[smoke, gas leaks].
•FR-1.4:Upon detection of an environmental hazard (per FR-1.3), the system shall alert the
Primary User and initiate an emergency alert sequence.
•FR-1.5:The emergency alert sequence (per FR-1.2, FR-1.4) shall dispatch notifications to,
in order of priority: [1. Caregivers, 2. Emergency Services].
•FR-1.6:The alert notification shall include the user’s identity, location, and the nature of the
event (e.g., “Fall Detected,” “Smoke Detected”).
6.1.2 Health & Wellness Management
These requirements define the system’s functions related to user health monitoring and assistance.
•FR-2.1:The system shall be capable of collecting and securely recording the Primary User’s
vital signs (e.g., heart rate, blood oxygen, sleep patterns, stress levels).
•FR-2.2:The system shall generate periodic and on-demand health reports from recorded vital
sign data.
•FR-2.3:The system shall provide a secure interface for authorized [Healthcare Professionals]
to access generated health reports (per FR-2.2).
•FR-2.4:The system shall allow the Primary User or Caregiver to create, update, and manage
medication schedules.
16
•FR-2.5:The system shall provide timely, multimodal reminders to the Primary User at
scheduled medication times.
•FR-2.6:The system shall provide a medical query interface (RAG) to answer user questions
with verified medical information.
6.1.3 Multimodal Interaction & Companionship
These requirements define the core AI interaction modalities and capabilities.
•FR-3.1:The system shall accept and process user commands via three distinct modalities:
[1. Spoken Language, 2. Sign Language (SL), 3. Touch Interface].
•FR-3.2:The system shall provide a conversational AI agent for the purpose of companionship
and emotional support.
•FR-3.3:The system shall analyze the Primary User’s communication (e.g., voice tone, facial
expression) to detect their emotional state.
•FR-3.4:The system shall adapt its conversational responses and tone based on the detected
emotional state (per FR-3.3).
6.1.4 Autonomous Navigation & Visual Assistance
These requirements govern the “rover” aspect of the system: its movement and environmental aware-
ness.
•FR-4.1:The system shall autonomously navigate a pre-mapped indoor environment from
point A to point B.
•FR-4.2:The system shall detect and avoid obstacles in its path, including static (e.g., furni-
ture) and dynamic (e.g., people, pets) objects.
•FR-4.3:The system shall provide a “follow” mode to autonomously follow the Primary User.
•FR-4.4:The system shall adhere to “traffic rules” (e.g., respecting boundaries, speed limits)
as defined by the user.
•FR-4.5:The system shall, upon request, provide visual assistance to the Primary User by:
– FR-4.5.1:Identifying and announcing specific objects.
– FR-4.5.2:Reading printed or digital text.
– FR-4.5.3:Providing a verbal description of the immediate scene.
6.1.5 User & System Management
These requirements define the administrative and support functions of the system.
•FR-5.1:The system shall securely authenticate all actors [Primary User, Caregiver, Healthcare
Professional, Administrator] before granting access to system functions or data.
•FR-5.2:The system shall allow the Primary User to manage their data privacy and sharing
preferences.
•FR-5.3:The system shall provide a remote dashboard for authorized [Caregivers] to view the
Primary User’s real-time status, location, and recent alerts.
17
•FR-5.4:The system shall support multiple, distinct Primary User profiles within a single
household, with personalized settings for each.
•FR-5.5:The system shall allow authorized [Administrators] to perform system maintenance,
apply software updates, and manage AI models.
6.2 Non-Functional Requirements
NovaBot’s non-functional requirements define the quality attributes and constraints that ensure the
system operates reliably, securely, and accessibly. These requirements span performance, reliability,
security, usability, scalability, safety, compatibility, data quality, and ethical considerations.
6.2.1 Performance Requirements
These requirements ensure NovaBot operates efficiently and responds in real time.
•NFR-1.1:The system shall process voice or sign-language commands and generate responses
within 2 seconds under normal operating conditions.
•NFR-1.2:The fall detection and hazard detection modules shall trigger alerts within 1 second
of event detection.
•NFR-1.3:The autonomous navigation system shall maintain a location update rate of at
least 10 Hz for smooth motion and obstacle avoidance.
•NFR-1.4:The system shall support continuous operation for at least 6 hours on a single full
battery charge under mixed-use conditions.
•NFR-1.5:All cloud communications (alerts, logs, dashboard updates) shall be transmitted
within 3 seconds of event creation.
6.2.2 Reliability and Availability
Ensures NovaBot’s stability and resilience in continuous use.
•NFR-2.1:The system shall achieve an uptime of 99% during normal operation hours.
•NFR-2.2:The system shall be able to recover automatically from transient failures (e.g.,
Wi-Fi disconnects, sensor faults) within 10 seconds.
•NFR-2.3:In the event of a component failure (e.g., camera or motor), the system shall grace-
fully degrade functionality while maintaining safety-critical features (e.g., emergency alerts).
•NFR-2.4:All critical logs (health, safety, navigation) shall be preserved locally for at least 7
days in case of network outage.
6.2.3 Security Requirements
Defines data protection, authentication, and privacy measures.
•NFR-3.1:All communication between NovaBot and the cloud shall use end-to-end encryption
(TLS 1.3 or higher).
•NFR-3.2:User authentication shall use multi-factor authentication (MFA) for caregivers and
administrators.
18
•NFR-3.3:Personal data and health information shall be stored in compliance with GDPR /
HIPAA-like standards for confidentiality.
•NFR-3.4:The system shall restrict access to user data based on role-based access control
(RBAC).
•NFR-3.5:All sensitive data stored locally shall be encrypted at rest (AES-256).
6.2.4 Usability and Accessibility
Ensures the system is intuitive and inclusive for users with disabilities.
•NFR-4.1:The interface shall be designed following WCAG 2.1 accessibility standards, sup-
porting users with visual, hearing, or mobility impairments.
•NFR-4.2:All user interactions shall provide multimodal feedback (visual + auditory + text)
to confirm actions.
•NFR-4.3:The system shall offer adaptive UI scaling for low-vision users.
•NFR-4.4:Voice and sign-language recognition accuracy shall be≥90% under normal indoor
conditions.
•NFR-4.5:The system shall provide language localization (multi-language support) for diverse
users.
6.2.5 Scalability and Maintainability
Ensures that NovaBot can evolve, expand, and be easily maintained.
•NFR-5.1:The modular architecture shall allow new AI models or sensors to be integrated
without redesigning the full system.
•NFR-5.2:The cloud backend shall support up to 100 concurrent connected devices without
performance degradation.
•NFR-5.3:Software updates shall be deployable over-the-air (OTA) without requiring manual
intervention.
•NFR-5.4:All code modules shall follow ROS and API documentation standards for main-
tainability.
•NFR-5.5:The system shall log all key events for debugging and allow remote diagnostics
through the guardian dashboard.
6.2.6 Safety and Compliance
Ensures that the system meets physical and operational safety standards.
•NFR-6.1:The rover shall not exceed 1 m/s indoors to ensure user and property safety.
•NFR-6.2:Obstacle avoidance accuracy shall be≥95% within a 1-meter detection range.
•NFR-6.3:The system shall meet ISO 13482 (safety requirements for personal care robots)
compliance standards where applicable.
•NFR-6.4:In the event of critical malfunction, the rover shall automatically stop and enter
safe mode.
19
6.2.7 Portability and Compatibility
Ensures flexibility across hardware and platforms.
•NFR-7.1:The software stack shall be compatible with both NVIDIA Jetson and Raspberry
Pi 5 architectures.
•NFR-7.2:The web dashboard shall be compatible with modern browsers (Chrome, Edge,
Safari) and mobile devices.
•NFR-7.3:The backend APIs shall follow RESTful or WebSocket standards for easy third-
party integration.
6.2.8 Data Quality and Accuracy
Ensures dependable sensor and AI inference results.
•NFR-8.1:The fall detection system shall achieve at least 95% detection accuracy and<5%
false-positive rate.
•NFR-8.2:Emotion recognition accuracy shall not fall below 85% under controlled lighting.
•NFR-8.3:Object detection latency shall not exceed 200 ms per frame on the edge processor.
6.2.9 Ethical and Privacy Requirements
Ensures user dignity, transparency, and responsible AI use.
•NFR-9.1:The AI assistant shall clearly inform the user when recording audio or video data.
•NFR-9.2:The system shall not transmit or store sensitive biometric data without explicit
consent.
•NFR-9.3:AI models shall be trained and used in compliance with ethical AI principles,
avoiding bias or discrimination.
6.3 Business Requirements
NovaBot’s business requirements define the strategic goals, stakeholder value propositions, and or-
ganizational objectives that guide the project. These requirements ensure alignment with market
needs, user expectations, and the university’s mission in assistive technology innovation.
6.3.1 Market & Social Impact Requirements
BR-M1: Accessibility Gap Closure
The system shall demonstrate a viable solution to the fragmentation problem identified in the prob-
lem statement, providing Primary Users with a unified, multimodal platform that reduces depen-
dency on multiple separate assistive devices and caregivers.
BR-M2: Inclusive Design Demonstration
The system shall exemplify human-centered design principles and inclusive technology, serving as a
reference implementation for assistive robotics in developing regions and underserved communities
where accessibility solutions remain limited.
20
BR-M3: Quality of Life Enhancement
The system shall measurably improve key quality-of-life indicators for target users, including inde-
pendence, social engagement, emotional well-being, and safety—as validated through user feedback
and comparative studies where applicable.
BR-M4: Addressable Market Recognition
The project shall identify and document a clear target market (e.g., elderly populations, disabled
individuals in rehabilitation centers, at-home care scenarios) with quantifiable demand to support
potential future commercialization or licensing.
6.3.2 Stakeholder Value Requirements
BR-S1: Primary User Empowerment
The system shall enable Primary Users to achieve increased autonomy and independence in daily ac-
tivities, specifically in communication, mobility, health monitoring, and emergency response—without
diminishing human dignity or social connection.
BR-S2: Caregiver Support
The system shall reduce caregiver burden through automated monitoring, remote alerts, and task
assistance, allowing caregivers to focus on emotional support and complex care decisions rather than
routine surveillance.
BR-S3: Healthcare Professional Integration
The system shall provide healthcare professionals with secure, actionable health data and insights,
enabling evidence-based care decisions and facilitating remote patient monitoring where applicable.
BR-S4: Guardian Confidence & Trust
The system shall establish trust through transparent operation, clear communication, explainable
AI decisions, and demonstrated reliability—earning confidence from guardians and family members
who oversee the Primary User’s welfare.
6.3.3 Technical Excellence & Innovation Requirements
BR-T1: Integrated AI Showcase
The system shall demonstrate state-of-the-art integration of multiple AI disciplines (NLP via LLM,
computer vision, emotion recognition, autonomous navigation) into a cohesive, real-time robotic
platform—advancing the team’s technical expertise and portfolio.
BR-T2: Edge AI Deployment
The system shall validate efficient edge computing practices by running complex AI models (object
detection, emotion recognition, NLP inference) on embedded hardware (Jetson/Raspberry Pi) within
strict latency and power budgets.
BR-T3: Modular Architecture Validation
The system shall prove the viability of a modular, extensible architecture that allows independent
development, testing, and integration of AI modules, sensors, and cloud services—facilitating future
enhancements and third-party integrations.
21
BR-T4: Real-Time Multimodal Performance
The system shall validate the feasibility of processing and responding to multimodal user inputs
(speech, sign language, touch) in real time on resource-constrained platforms, setting performance
benchmarks for future assistive robotics systems.
6.3.4 Scalability & Sustainability Requirements
BR-SC1: Prototype-to-Product Pathway
The system architecture and development process shall be designed to facilitate transition from
academic prototype to deployable product, with clear documentation, modular code, and identified
scalability bottlenecks and mitigation strategies.
BR-SC2: Multi-Device Scalability
The cloud backend and communication infrastructure shall be architected to support multiple con-
current NovaBot instances (initially prototyped with 1-5 units, designed for 100+ in future iterations)
without degradation in performance or reliability.
BR-SC3: Technology Stack Sustainability
The chosen technologies (ROS, TensorFlow, PostgreSQL, Node.js) shall have active community sup-
port, documented best practices, and clear upgrade paths to ensure long-term maintainability and
evolution of the system.
BR-SC4: Cost Optimization
The system shall demonstrate cost-effectiveness through leveraging affordable, commercial-off-the-
shelf hardware components (Jetson Nano, Raspberry Pi, JetAuto kit) rather than custom-built
solutions, validating a sustainable business model for developing regions.
6.3.5 Educational & Research Contribution Requirements
BR-E1: Interdisciplinary Learning Platform
The project shall serve as a comprehensive, hands-on learning platform for the team, integrating
concepts from AI/ML, robotics, embedded systems, cloud computing, HCI, and project manage-
ment—demonstrating mastery of modern software engineering practices.
BR-E2: Research Publication Opportunities
The project shall generate research contributions worthy of academic publication, particularly in
areas such as multimodal AI for accessibility, lightweight emotion recognition on edge devices, or
ASL recognition in real-world settings.
BR-E3: Open-Source Potential
The system design and documentation shall be structured to enable potential open-sourcing of key
modules (e.g., ASL recognition pipeline, fall detection algorithm, navigation stack) to benefit the
broader assistive robotics and open-source community.
22
BR-E4: Case Study & Best Practices Documentation
The project shall produce comprehensive documentation suitable for publication as a technical case
study, detailing architecture decisions, lessons learned, performance metrics, and best practices for
multi-AI-model integration on embedded systems.
6.3.6 Regulatory, Safety & Compliance Requirements
BR-R1: Safety-Critical System Certification
The system shall demonstrate compliance with relevant safety standards for personal care robots (ISO
13482 where applicable) and maintain a safety-first design philosophy throughout development, with
rigorous testing for edge cases and failure modes.
BR-R2: Data Privacy & Regulatory Compliance
The system shall be architected to support GDPR, HIPAA-like, and evolving regional data protection
standards, ensuring that user health and biometric data collection, storage, and sharing align with
legal requirements and ethical principles.
BR-R3: Ethical AI Governance
The system shall implement transparent, auditable AI decision-making processes and establish clear
governance for training data, model bias mitigation, and user consent—demonstrating responsible
AI practices suitable for vulnerable populations.
BR-R4: Transparency & Accountability
The system shall explicitly inform users when recording data, using AI algorithms, or sharing in-
formation with guardians/professionals; provide explanations for system decisions; and maintain
detailed audit logs for accountability and regulatory review.
6.3.7 Strategic Positioning & Long-Term Vision Requirements
BR-V1: University Reputation & Mission Alignment
The project shall enhance Alamein International University’s reputation as a leader in AI, robotics,
and socially responsible technology innovation, aligning with the institution’s mission to support
underrepresented and disabled communities.
BR-V2: Industry Partnership Foundation
The project shall establish a foundation for potential partnerships with healthcare organizations,
NGOs, disability advocacy groups, or technology companies, demonstrating proof-of-concept for
real-world assistive solutions.
BR-V3: Student Employability & Career Growth
The project shall provide the team members with portfolio-quality deliverables, demonstrated ex-
pertise, and publication/presentation opportunities that enhance career prospects in AI, robotics,
and humanitarian technology sectors.
23
BR-V4: Innovation Sustainability
The project shall document technical and organizational insights that position the team or institution
to undertake subsequent phases (e.g., outdoor navigation, IoT integration, multi-user households)
or related research initiatives in assistive robotics.
6.4 Business Rules
Business rules define operational constraints, decision policies, and governance principles that shape
how NovaBot functions and evolves within its business and organizational context.
6.4.1 Authentication & Access Control
BR-01: User Authentication
Each user and guardian must register and authenticate before accessing NovaBot’s services. The
system validates identity and permissions for all interactions.
BR-02: Guardian Access Control
Guardians can only view and monitor the specific users linked to their accounts. Remote control
privileges are restricted for safety.
BR-13: Data Access Rights
Guardians have read-only access to data. Only the user can authorize control or configuration
changes.
6.4.2 Communication & Accessibility
BR-03: Accessibility Mode Detection
NovaBot automatically identifies or applies the user’s preferred communication mode (voice, text,
or sign language). The user may manually change the mode in settings.
BR-04: Fallback Communication Mode
If any input/output module fails (e.g., microphone or camera), NovaBot switches to the default text
interface to maintain accessibility.
6.4.3 Safety & Emergency Response
BR-05: Emergency Alert Rule
Upon detecting a fall or emergency, NovaBot must immediately alert the guardian dashboard with
live camera feed, timestamp, and location data.
BR-06: Obstacle and Safety Rule
NovaBot halts movement when an obstacle is detected within 0.5 meters and resumes only when the
path is clear. Safety overrides all user commands.
24
BR-07: User Following Distance
The rover must maintain a configurable following distance from the user (default 1.5 meters) to
ensure safe navigation.
BR-15: Fail-Safe Mode
In case of hardware or software malfunction, NovaBot enters a fail-safe mode that stops motion and
notifies the guardian.
6.4.4 Data Privacy & Security
BR-08: Data Privacy and Encryption
All user data (voice, video, logs) must be encrypted before storage or cloud upload. Data sharing
requires explicit consent.
6.4.5 AI Ethics & Behavior
BR-09: AI Ethical Interaction
The AI assistant must only respond in a supportive and safe manner. It cannot provide harmful,
biased, or non-assistive replies.
BR-10: Emotion-Based Adaptation
NovaBot may adjust its tone and responses based on detected emotions, but cannot override user
commands or perform unsolicited actions.
6.4.6 System Maintenance & Operations
BR-11: System Updates
Software and firmware updates are allowed only when NovaBot is idle and connected to a secure
network. Update logs must be saved automatically.
BR-12: Battery Management
The system issues low battery alerts below 15% and prevents task initiation if remaining power is
insufficient for completion.
BR-14: Operational Integrity
The robot must complete sensor calibration (LiDAR, camera, etc.) before initiating autonomous
movement.
25
Chapter 7
System Design
7.1 Use Case Diagram
use_case_diagram.png
Figure 7.1: NovaBot Use Case Diagram
26
7.2 Architecture Diagrams
7.2.1 High-Level Architecture
high_level_architecture.png
Figure 7.2: High-Level System Architecture
27
7.2.2 Composite Diagram
composite_diagram.png
Figure 7.3: Composite Structure Diagram
28
7.3 Data Flow Diagrams
7.3.1 Level 0 DFD
dfd_level0.png
Figure 7.4: Data Flow Diagram – Level 0
29
7.3.2 Level 1 DFD
dfd_level1.png
Figure 7.5: Data Flow Diagram – Level 1
30
7.4 Sequence Diagrams
sequence_diagram.png
Figure 7.6: Sample Sequence Diagram
7.5 Class Diagram
7.6 Entity Relationship Diagram (ERD)
7.7 User Stories
7.8 Traceability Matrix
Requirement
ID
Description Design Component Test Case ID
REQ-1 Voice recognition Speech-to-text module TC-01
31
REQ-2 Object detection Vision module TC-05
32
Chapter 8
Testing Methodologies
8.1 Testing Requirements
8.2 Test Cases
8.3 Unit Testing
8.4 Integration Testing
8.5 System Testing
8.6 Acceptance Testing
8.7 Evaluation Metrics
8.8 Testing Tools
33
Chapter 9
Project Management
9.1 Timeline / Gantt Chart
9.2 Team Roles
9.3 Risk Management
34
Chapter 10
Ethical and Legal Considerations
10.1 Timeline / Gantt Chart
10.2 Team Roles
10.3 Risk Management
35
Chapter 11
Conclusion
36
Chapter 12
References
37
