{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Audio Emotion Training - Wav2Vec 2.0 on RAVDESS\n",
    "\n",
    "Train Wav2Vec 2.0 for speech emotion recognition. Run in Google Colab with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets librosa soundfile -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport numpy as np\nimport torch\nimport librosa\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor, TrainingArguments, Trainer\nfrom torch.utils.data import Dataset\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAVDESS emotion mapping (filename format: Actor_XX/03-01-EMOTION-...)\n# Emotions: 01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=disgust, 08=surprised\nEMOTION_MAP = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fearful', 7:'disgust', 8:'surprised'}\nLABEL2ID = {v:k-1 for k,v in EMOTION_MAP.items()}\nID2LABEL = {k-1:v for k,v in EMOTION_MAP.items()}\nSAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare RAVDESS dataset\nDATA_PATH = '../data/audio_data'  # Adjust path\n\ndef get_ravdess_files(data_path):\n    files, labels = [], []\n    for actor_dir in Path(data_path).glob('Actor_*'):\n        for wav in actor_dir.glob('*.wav'):\n            emotion_code = int(wav.stem.split('-')[2])\n            files.append(str(wav))\n            labels.append(emotion_code - 1)  # 0-indexed\n    return files, labels\n\naudio_files, labels = get_ravdess_files(DATA_PATH)\nprint(f'Found {len(audio_files)} audio files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(audio_files, labels, test_size=0.2, stratify=labels, random_state=42)\nprint(f'Train: {len(X_train)}, Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\nclass AudioDataset(Dataset):\n    def __init__(self, files, labels, feature_extractor):\n        self.files, self.labels = files, labels\n        self.fe = feature_extractor\n    \n    def __len__(self): return len(self.files)\n    \n    def __getitem__(self, idx):\n        wav, _ = librosa.load(self.files[idx], sr=SAMPLE_RATE)\n        inputs = self.fe(wav, sampling_rate=SAMPLE_RATE, return_tensors='pt', padding=True)\n        return {'input_values': inputs.input_values.squeeze(), 'labels': torch.tensor(self.labels[idx])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and feature extractor\nMODEL = 'facebook/wav2vec2-base'\nfe = Wav2Vec2FeatureExtractor.from_pretrained(MODEL)\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(MODEL, num_labels=8, id2label=ID2LABEL, label2id=LABEL2ID)\n\ntrain_ds = AudioDataset(X_train, y_train, fe)\ntest_ds = AudioDataset(X_test, y_test, fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for variable length audio\ndef collate_fn(batch):\n    inputs = [b['input_values'] for b in batch]\n    labels = torch.stack([b['labels'] for b in batch])\n    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n    return {'input_values': inputs, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\nargs = TrainingArguments(\n    output_dir='./wav2vec_audio',\n    num_train_epochs=10,\n    per_device_train_batch_size=4,  # Small batch for memory\n    per_device_eval_batch_size=4,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    fp16=True,\n    gradient_accumulation_steps=4\n)\n\ntrainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=test_ds, data_collator=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\ntrainer.save_model('../models/wav2vec_audio')\nfe.save_pretrained('../models/wav2vec_audio')\nprint('Model saved!')"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
